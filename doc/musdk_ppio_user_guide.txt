.. _ppv2_user_guide:

Packet Processor V2 (PPV2) User Guide
=====================================
Introduction
------------

MUSDK PPV2 driver and applications are part of the MUSDK package.
MUSDK PPV2 driver provides capability of usage of the Packet Processing HW Engine (PPv2.2)
directly from user-space applications with minimum overhead and high performance.


Kernel Dependencies
~~~~~~~~~~~~~~~~~~~
The MUSDK PPV2 driver depends on the KS driver for following purposes:
	- Read DTS file and extract all common HW resources information as well as all the KS specific HW resources (e.g. HIFs, BM-Pools, RxQs, etc.).
	- Initialize and maintain PHY and link state for all NICs (both KS and US), and expose API’s (ioctl/ethtool/sysfs) for NIC and Phy functionality on the MUSDK ports.

Resource Mapping
~~~~~~~~~~~~~~~~

PPV2
^^^^
The PPV2 HW has a number of resources that are shared between MUSDK and the Kernel: HIF, BPOOL, RSS_TABLES and POLICERS.
It is up to the user application to identify which resources are used by the Kernel. These resources must not be used by MUSDK.

The pp2_init() API provides a set of fields to supply ‘reserved resource maps’ (e.g. reserved by Kernel) for each of the above resources. MUSDK later on uses these ‘reserved resource maps’ to validate resource availability during the Init API functions for each resource (i.e. pp2_hif_init(), pp2_bpool_init(), etc.).

Objects Overview
~~~~~~~~~~~~~~~~

     .. _fig_musdk_ppv2:

     .. figure:: /images/musdk_ppv2_intro.*
        :align: center


Intro
^^^^^
	- Any Marvell SoC containing PPV2 HW may have a number of **'PPV2 HW Instances'**.
	- i.e. The A7040 has 1 'PPV2 HW Instance', and A8040 has 2 'PPV2 HW Instances'.

	- A **'PPV2 HW Instance'** is *not* a MUSDK object, it is however an important concept since some of the MUSDK objects are
	  local to a specific 'PPV2 HW Instance'.

	- For these types of objects, the 'PPV2 HW Instance' number is *specified as part of the object's name*
	  during the object Init.
	- The **pp2_get_num_inst() API** allows the user application to learn the number of 'PPV2 HW Instances' existing in the
	  system.

.. _hif:

HIF
^^^
A Host-Interface (HIF) object is required to access BM-Pools and TX-Qs for run-time operations.
A HIF objects exists across *all* 'PPV2 HW Instances', meaning that in the MUSDK implementation it is physically instantiated in each of the 'PPV2 HW Instances'.

Operations that require a HIF:
	- Putting and getting buffers to/from a BM-Pool.
	- Enqueuing packets to TX-Qs.

HIF Behavior
	- Each HIF must only be used in a single context simultaneously. *This is a hard requirement*,
	  that for performance considerations *is not enforced in MUSDK*.
	- Allocating a HIF for each CPU thread is recommended, in order to prevent the need to lock access to the HIF
	  in the user application.
	- Maximum number of HIF object is limited to 9 by hardware.
	- HIFs are also used in the kernel, and the HIF resource must be managed by the user application.
	- In the current kernel driver, by default the first 4 HIFs are used.
	  This can be reduced to 1 with SINGLE_RESOURCE_MODE queue_mode (refer to kernel :ref:`queue_mode module parameter <queue_mode>` section).
	  The user application should reserve these, by adding them to the 'hif_reserved_map' parameter in the pp2_init() API.


BPOOL
^^^^^
	- The BPOOL object is required to make use of the PPV2 HW Buffer Management, which is the mandatory Buffer Manager
	  for PPV2 PPIO's.
	- A BPOOL object is created on a certain 'PPV2 HW Instance'.
	- The PPIO and the BPOOL object have a 'many-to-many' relationship, meaning that a PPIO Object can use multiple BPOOLs,
	- and a BPOOL object may be shared by multiple PPIO's. The BPOOL and PPIO must reside in the same 'PPV2 HW Instance'.



PPIO
^^^^
	- The PPIO object is the MUSDK presentation of an interface.
	- A PPIO object is a logical port capable of receiving and transmitting packets.
	- As a minimum, a PPIO has a collection of RX queues (RX-Qs) and TX queues (TX-Qs), at least one of each.
	  In addition, a PPIO is associated with one or more BPOOLS (See TC below).

**PPIO TC** - Traffic Class (TC) is an important sub-structure of the PPIO. Multiple TC's enable multiple priority levels in the RX direction in the PPIO. The user application may configure which traffic is sent to each TC by using the PPV2 Classifier, and choose to empty the RX-Qs associated with one TC before the RX-Qs of another TC.
A TC consists of a group of RX-Qs, and the BPools used by those RX-Qs. A TC may be associated with up to 2 BPOOLs. RX-Qs within the TC are selected by the PPIO's hash function.


  .. Note::
	When a TC is associated with multiple BPools, the specific BPOOL for each incoming packet is selected by HW, according to
	the best size match.


Feature Set
-----------

Supported APIs
~~~~~~~~~~~~~~

PP2:
	- Support all API's
HIF:
	- Support all API's
BPOOL:
	- Support all API's
	- pp2_bpool_put_buffs() and pp2_bpool_put_buff() both exist.
	  Currently both are supported, pp2_bpool_put_buff() is to be phased out.

PPIO:
	- Init, Filtering, Statistics:
		- Support all API's *excluding* :
			- pp2_ppio_flush_vlan
	- Send/Receive:
		- Support all API's *excluding* :
			- pp2_ppio_send_sg
	- Outqueue:
		- Support all pp2_ppio_outq_xxx API's, *excluding* :
			- pp2_ppio_outq_desc_set_dsa_tag
			- pp2_ppio_get_outq_state
		- Support all pp2_ppio_inq_xxx API's, *excluding* :
			- pp2_ppio_inq_desc_get_pp_io
	- Link & NIC:
		- Support all API's *excluding* :
			- pp2_ppio_set_link_state


General Limitations
~~~~~~~~~~~~~~~~~~~

- When setting the tx descriptor, the length field needs to be written after the offset. Otherwise the length field is overwritten.

- The example applications do not perform MTU size checking. In case traffic is sent with MTU size bigger than the
  configured MTU size, packets will not be discarded but rather truncated.

Features Description
--------------------

Pad and CRC offloads
~~~~~~~~~~~~~~~~~~~~
Marvell PPv2.2 incorporates hardware support for packet CRC offload on Rx (CRC validation) and TX (CRC generation).
Pad and CRC is always enabled by the PPIO driver.

On receive side, padding is removed and CRC is validated automatically by the PPv2.2 hardware.
Therefore, MUSDK users receive frames without padding and CRC.

On transmit side, appropriate padding is appended to the frame, and CRC is calculated and added to the ETH frame.
Applications do not need to transmit frames with pad or CRC.

Checksum offload
~~~~~~~~~~~~~~~~
Marvell PPv2.2 incorporates hardware support for L3 and L4 checksum offloads.

On receive side, checksum is validated automatically by the PPv2.2 hardware. An error indication will be raised as part
of the Rx descriptor.

In transmit side, if MUSDK client SW would like to utilize this offload, appropriate fields in the Tx descriptor should be set using the appropriate API. The PPv2.2 HW will than calculate and generate L3 and/or L4 CSum.

.. _rss_support:

Multicore Support for RSS
~~~~~~~~~~~~~~~~~~~~~~~~~
RSS is a statistical load balancing technique for distributing packet flows that arrive on the same ethernet port
between the cores of a multi-core CPU. Packets of the same flow are always directed to the same queue and CPU, and
therefore order within flows is preserved.
The load balancing improves the performance of Symmetrical Multi-Processing (SMP) cores.

RSS selects an RSS-queue according to key hash, which is mapped to an RX queue by configuration.

The supported hash types are as follows :
	- 2-tuple: IP source address and IP destination address (IPv4 and IPv6) or
	- 5-tuple: IP source address, IP destination address, IP protocol (TCP/UDP only), L4 source port, L4 destination
		   port (IPv4 and IPv6)

	- RSS is enabled by setting the following parameters in MUSDK API:
		- hash_type parameter in pp2_ppio_inqs_params struct. Possible values are:
			- PP2_PPIO_HASH_T_NONE:		Invalid hash type (hashing mechanism is disabled)
			- PP2_PPIO_HASH_T_2_TUPLE:	Hash performed on IP source and IP destination fields
			- PP2_PPIO_HASH_T_5_TUPLE:	Hash performed on IP source, IP destination, IP protocol, L4 source and L4 destination

		- num_in_qs in pp2_ppio_tc_params struct. This parameter defines the number of RSS queues as follows:
			- Value of 1: one RSS-queue is defined in the traffic class (TC) and all incoming traffic is directed to this queue
			- Value of 2: two RSS-queues are defined in the TC and all incoming traffic is distributed between these 2 queues
			- Value of 3: three RSS-queues are defined in the TC and all incoming traffic is distributed between these 3 queues
			- Value of 4: four RSS-queues are defined in the TC and all incoming traffic is distributed between these 4 queues

Limitations:
	- All traffic is equally weighted between the defined RSS queues (no option to define weighted RSS queues)

Please refer to MUSDK classifier section "How To Run The Example Application" for an explanation of how to use and configure this API.

.. _mac_filtering:

MAC address filtering
~~~~~~~~~~~~~~~~~~~~~
MAC filtering is implemented by PPv2.2 hardware. MAC filtering is done in a dedicated parser &
L2 filtering TCAM by parsing the DMAC field.

- Filtering capability: For each MUSDK port, it is possible to configure up to 25 UC and MC filtering entries.
- By default, the maximum number of UC entries is 4, and the maximum number of MC entries is 21( 25 - max UC entries).
- It is possible to change the maximum number of UC entries by configuring the uc_filter_max module parameter in the
  kernel driver. Please refer to PPv2.2 Kernel Module Parameters chapter.
- MAC filtering behavior: The filtering behavior is defined as follows:
	- UC filtering: If UC entries > max UC entries (default is 4), the port is set to unicast promisc mode.
	- MC filtering: If MC entries > (25- max UC entries), the port is set to all-multi mode.
	- From the behavior definition, there are 4 states for each port:
		- State1: non-unicast promisc & non-allmulti
		- State2: unicast promisc mode & non-allmulti
		- State3: non-unicast promisc & allmulti
		- State4: unicast promisc & allmulti

Please refer to MUSDK classifier section "How To Run The Example Application" for an explanation of how to use and configure this API.

.. _vlan_filtering:

VLAN filtering
~~~~~~~~~~~~~~
VLAN filtering is implemented by PPv2.2 hardware. VLAN filtering is done in a dedicated parser &
L2 filtering TCAM by parsing the VLAN ID field.

- Filtering capability: For each MUSDK port, it is possible to configure up to 10 VLAN ID filtering entries.
- VLAN filtering behavior:
	- If no VLAN filtering entries are set, the system allows passing through all VLAN traffic.
	- Whenever one or more VLAN ID filters is configured, all VLAN traffic matching one of the VLAN ID's will pass
	  through the system, while all VLAN traffic not matching the configured VLAN ID's will be dropped.
	- All non-VLAN traffic will pass through the system independently of the VLAN filter configured.

Please refer to MUSDK classifier section "How To Run The Example Application" for an explanation of how to use and configure this API.

Promiscuous and multicast mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- When promisc mode is set (ifconfig +promisc) , the port is set to promisc mode and all unicast and multicast traffic will pass through.
- When promisc mode is unset (ifconfig -promisc), the port is set to normal mode and only unicast and multicast traffic defined in MAC address filtering will pass through.
- When all-multi mode is set (ifconfig +allmulti) , the port is set to all multicast mode and all unicast and multicast traffic will pass through.
- When all-multi mode is unset (ifconfig -allmulti), the port is set to normal mode and only multicast traffic defined in MAC address filtering will pass through.

Please refer to MUSDK classifier section "How To Run The Example Application" for an explanation of how to use and configure this API.

Statistics
~~~~~~~~~~
HW counters are 16/32-bit.
Counters that are maintainable by nature, are 64-bit in MUSDK.

"Maintainable counters" are counters for which MUSDK can guarantee the HW counter will not reach U32_MAX/U16_MAX,
assuming "statistic_maintenance" is performed.
The other (non-maintainable) counters will latch at U32_MAX/U16_MAX, until read by an API statistics call.

The "maintain_stats" parameter, which is provided during pp2_ppio_init() API determines
if periodic maintenance (triggered by internal threshold) will be performed during pp2_ppio_recv()/pp2_ppio_send() functions.

- pp2_ppio_inq_get_statistics():
	- 'enq_desc' counter is 64-bit, other inq_counters are 16/32-bit.

- pp2_ppio_outq_get_statistics():
	- all counters are 64-bit.

Logical port
~~~~~~~~~~~~
The MUSDK logical port is based on the concept where a NIC is being shared between Kernel Space (KS) and User Space (US).
In this case, the basic NIC will be handled by the KS driver while the US will be introduced as a logical-port.
i.e. only some traffic flows will be directed according to a specific dedicated classification stage to a group of
queues that will be directed towards the US driver.

The MUSDK API allows configuring a logical port as following:

	- The logical port parameters are configured when opening a PP-IO object (packet processor interface).
		- This is performed through the MUSDK logical port API (defined in mv_pp2_ppio.h file, struct pp2_ppio_log_port_rule_params).

	- The parameters provided in MUSDK API for logical port include a list of specific protocols which the software needs to route differently between KS and US.

	- The API also allows the following configuration flexibility:
		- Define whether the traffic will be forwarded to KS and specific protocols to US, or the inverse (traffic will be forwarded to US and specific protocols to US)
			- PP2_CLS_TARGET_LOCAL_PPIO	-> default traffic going to kernel and logical port to MUSDK
			- PP2_CLS_TARGET_OTHER		-> default traffic going to MUSDK and logical port to kernel

		- Define classification according to different network protocols (defined in mv_net.h API file)
			Supported protocols:
				- MV_NET_PROTO_VLAN
				- MV_NET_PROTO_PPPOE
				- MV_NET_PROTO_IP
				- MV_NET_PROTO_IP4
				- MV_NET_PROTO_IP6
				- MV_NET_PROTO_TCP
				- MV_NET_PROTO_UDP
				- MV_NET_PROTO_ICMP
				- MV_NET_PROTO_ARP

		- Define classification also according to special bits in a protocol
			Supported special protocols fields:
				- MV_NET_PROTO_ETH_DSA
			Supported fields:
				- MV_NET_ETH_F_DSA_TAG_MODE

		- Define whether the classification will be performed on a specific protocol or the negated protocol
			i.e. send all UDP traffic to US or send all non-UDP traffic to US

		- It is possible to configure more than one set of rules to match
			NOTE: in current release only one set is supported

Please refer to MUSDK classifier section "How To Run The Example Application" for an explanation of how to use and configure this API.

Policing
~~~~~~~~~~~~~~
Policing is implemented by PPv2.2 hardware.

The Policer applies bandwidth control, according to RFC 2697, over the flows that are metered. A flow that does not confirm to the subscribed policy (bandwidth and burst size)
is marked and may be dropped as a result.
Incoming packets to the Policer are already marked by the classifier. In color aware mode, the Policer uses the input color, and in color blind mode,
it ignores the input color Each input packet is marked by the Policer as either green, yellow, or red. The marking is based on the Committed Information.
Rate (CIR) and 2 associated burst sizes:
* A Committed Burst Size (CBS)
* An Excessive Burst Size (EBS).
A packet is marked:
* Green if it does not exceed the CBS
* Yellow if it exceed the CBS but does not exceed the EBS
* Red otherwise

- For each MUSDK port, it is possible to configure one policer-profile.
- A policer-profile can only be attached to one MUSDK port.
- Both regular and logical port can be attached with policer-profile.
- Initial policer profile may be configured during port initialization.
- The policer profile may be assigned using classifier API (see 'musdk_cls_user_guide').
- For each TC user can set initial color that may be used as the input color for the policer. the default color is GREEN.

Egress Scheduling and Rate-Limiting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Egress scheduling and rate limiting are implemented by PPv2.2 hardware.

The egress scheduler is working in such a way that when multiple egress queues have pending packets, the order in which
they are transmitted can be chosen to be either by fixed priority, where packets in queues with lower IDs are
transmitted before those with higher IDs, or using a weighted round robin (WRR) scheme, where packets are
scheduled from all queues, in correlation to their configured weights. When some queues are set to the fixed
priority mode and some to WRR mode, fixed priority packets have precedence over WRR ones. The weight range is
between 1 and 255 and WRR is the default mode.

The rate-limiter applies bandwidth control according to 'single-rate' with Committed Information Rate (CIR) and Committed Burst Size (CBS).
The maximum transmission rate can be limited for both individual egress queues and for entire ports.
These rates have to be at least 100kbps, and can only be exceeded for short bursts of data with configurable size. The minimum
of the burst is 64kB.

Scheduler and Rate limiter parameters are set during port initialization using the pp2_ppio_params struct and cannot be modified in runtime.

Loopback
~~~~~~~~
Marvell PPv2.2 incorporates hardware support for loopback operation.

If an interface operates in loopback mode, all TX traffic is looped back into the RX side
